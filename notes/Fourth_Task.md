# æ‰‹æŠŠæ‰‹ä¸€èµ·å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘ğŸ’¯(åˆçº§ç‰ˆ)

***

<img src="../picts/image_20240618095023600.png" alt="image_20240618100050682"  />

***

## å­¦ä¹ å‚è€ƒ

é¡¹ç›®æ•™ç¨‹ï¼š[ä¸­æ–‡æ•™ç¨‹](https://datawhalechina.github.io/llm-universe/)

ä»£ç ä»“åº“ï¼š[ä»£ç åœ°å€](https://github.com/datawhalechina/llm-universe)

<font color="blue">ä»“åº“ä»£ç ç›®å½•è¯´æ˜ï¼š</font>

>```markup
>requirements.txtï¼šå®˜æ–¹ç¯å¢ƒä¸‹çš„å®‰è£…ä¾èµ–
>notebookï¼šNotebook æºä»£ç æ–‡ä»¶
>docsï¼šMarkdown æ–‡æ¡£æ–‡ä»¶
>figuresï¼šå›¾ç‰‡
>data_baseï¼šæ‰€ä½¿ç”¨çš„çŸ¥è¯†åº“æºæ–‡ä»¶
>```



<img src="../picts/image-20240618100050682.png" alt="image-20240618100050682" style="zoom:80%;margin-left:0px;" />

## ç¬¬å››ç«  æ„å»ºRAGåº”ç”¨

### é¡¹ç›®ç®€ä»‹

æœ¬æ¬¡è¯¾ç¨‹å­¦ä¹ ä¸»è¦æ˜¯é¢å‘å°ç™½å¼€å‘è€…çš„å¤§æ¨¡å‹åº”ç”¨å¼€å‘å­¦ä¹ ï¼Œæ—¨åœ¨åŸºäºé˜¿é‡Œäº‘æœåŠ¡å™¨ï¼Œç»“åˆä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œé€šè¿‡ä¸€ä¸ªè¯¾ç¨‹å®Œæˆå¤§æ¨¡å‹å¼€å‘çš„é‡ç‚¹å…¥é—¨ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬å¦‚ä¸‹çš„äº”éƒ¨åˆ†å†…å®¹ï¼š

1. å¤§æ¨¡å‹ç®€ä»‹ï¼Œä½•ä¸ºå¤§æ¨¡å‹ã€å¤§æ¨¡å‹ç‰¹ç‚¹æ˜¯ä»€ä¹ˆã€LangChain æ˜¯ä»€ä¹ˆï¼Œå¦‚ä½•å¼€å‘ä¸€ä¸ª LLM åº”ç”¨ï¼Œé’ˆå¯¹å°ç™½å¼€å‘è€…çš„ç®€å•ä»‹ç»ï¼›
2. å¦‚ä½•è°ƒç”¨å¤§æ¨¡å‹ APIï¼Œæœ¬èŠ‚ä»‹ç»äº†å›½å†…å¤–çŸ¥åå¤§æ¨¡å‹äº§å“ API çš„å¤šç§è°ƒç”¨æ–¹å¼ï¼ŒåŒ…æ‹¬è°ƒç”¨åŸç”Ÿ APIã€å°è£…ä¸º LangChain LLMã€å°è£…ä¸º Fastapi ç­‰è°ƒç”¨æ–¹å¼ï¼ŒåŒæ—¶å°†åŒ…æ‹¬ç™¾åº¦æ–‡å¿ƒã€è®¯é£æ˜Ÿç«ã€æ™ºè°±AIç­‰å¤šç§å¤§æ¨¡å‹ API è¿›è¡Œäº†ç»Ÿä¸€å½¢å¼å°è£…ï¼›
3. çŸ¥è¯†åº“æ­å»ºï¼Œä¸åŒç±»å‹çŸ¥è¯†åº“æ–‡æ¡£çš„åŠ è½½ã€å¤„ç†ï¼Œå‘é‡æ•°æ®åº“çš„æ­å»ºï¼›
4. æ„å»º RAG åº”ç”¨ï¼ŒåŒ…æ‹¬å°† LLM æ¥å…¥åˆ° LangChain æ„å»ºæ£€ç´¢é—®ç­”é“¾ï¼Œä½¿ç”¨ Streamlit è¿›è¡Œåº”ç”¨éƒ¨ç½²
5. éªŒè¯è¿­ä»£ï¼Œå¤§æ¨¡å‹å¼€å‘å¦‚ä½•å®ç°éªŒè¯è¿­ä»£ï¼Œä¸€èˆ¬çš„è¯„ä¼°æ–¹æ³•æœ‰ä»€ä¹ˆï¼›

æœ¬é¡¹ç›®ä¸»è¦åŒ…æ‹¬ä¸‰éƒ¨åˆ†å†…å®¹ï¼š

1. LLM å¼€å‘å…¥é—¨ã€‚V1 ç‰ˆæœ¬çš„ç®€åŒ–ç‰ˆï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…æœ€å¿«ã€æœ€ä¾¿æ·åœ°å…¥é—¨ LLM å¼€å‘ï¼Œç†è§£ LLM å¼€å‘çš„ä¸€èˆ¬æµç¨‹ï¼Œå¯ä»¥æ­å»ºå‡ºä¸€ä¸ªç®€å•çš„ Demoã€‚
2. LLM å¼€å‘æŠ€å·§ã€‚LLM å¼€å‘æ›´è¿›é˜¶çš„æŠ€å·§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šPrompt Engineeringã€å¤šç±»å‹æºæ•°æ®çš„å¤„ç†ã€ä¼˜åŒ–æ£€ç´¢ã€å¬å›ç²¾æ’ã€Agent æ¡†æ¶ç­‰
3. LLM åº”ç”¨å®ä¾‹ã€‚å¼•å…¥ä¸€äº›æˆåŠŸçš„å¼€æºæ¡ˆä¾‹ï¼Œä»æœ¬è¯¾ç¨‹çš„è§’åº¦å‡ºå‘ï¼Œè§£æè¿™äº›åº”ç”¨èŒƒä¾‹çš„ Ideaã€æ ¸å¿ƒæ€è·¯ã€å®ç°æ¡†æ¶ï¼Œå¸®åŠ©åˆå­¦è€…æ˜ç™½å…¶å¯ä»¥é€šè¿‡ LLM å¼€å‘ä»€ä¹ˆæ ·çš„åº”ç”¨ã€‚

### é¡¹ç›®æ„ä¹‰

* LLM æ­£é€æ­¥æˆä¸ºä¿¡æ¯ä¸–ç•Œçš„æ–°é©å‘½åŠ›é‡ï¼Œå…¶é€šè¿‡å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå¼€å‘è€…æä¾›äº†æ–°çš„ã€æ›´å¼ºå¤§çš„åº”ç”¨å¼€å‘é€‰æ‹©ã€‚éšç€å›½å†…å¤–äº•å–·å¼çš„ LLM API æœåŠ¡å¼€æ”¾ï¼Œå¦‚ä½•åŸºäº LLM API å¿«é€Ÿã€ä¾¿æ·åœ°å¼€å‘å…·å¤‡æ›´å¼ºèƒ½åŠ›ã€é›†æˆ LLM çš„åº”ç”¨ï¼Œå¼€å§‹æˆä¸ºå¼€å‘è€…çš„ä¸€é¡¹é‡è¦æŠ€èƒ½ã€‚
* ç›®å‰ï¼Œå…³äº LLM çš„ä»‹ç»ä»¥åŠé›¶æ•£çš„ LLM å¼€å‘æŠ€èƒ½è¯¾ç¨‹å·²æœ‰ä¸å°‘ï¼Œä½†è´¨é‡å‚å·®ä¸é½ï¼Œä¸”æ²¡æœ‰å¾ˆå¥½åœ°æ•´åˆï¼Œå¼€å‘è€…éœ€è¦æœç´¢å¤§é‡æ•™ç¨‹å¹¶é˜…è¯»å¤§é‡ç›¸å…³æ€§ä¸å¼ºã€å¿…è¦æ€§è¾ƒä½çš„å†…å®¹ï¼Œæ‰èƒ½åˆæ­¥æŒæ¡å¤§æ¨¡å‹å¼€å‘çš„å¿…å¤‡æŠ€èƒ½ï¼Œå­¦ä¹ æ•ˆç‡ä½ï¼Œå­¦ä¹ é—¨æ§›ä¹Ÿè¾ƒé«˜ã€‚
* æœ¬é¡¹ç›®ä»å®è·µå‡ºå‘ï¼Œç»“åˆæœ€å¸¸è§ã€é€šç”¨çš„ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œæ·±å…¥æµ…å‡ºé€æ­¥æ‹†è§£ LLM å¼€å‘çš„ä¸€èˆ¬æµç¨‹ã€æ­¥éª¤ï¼Œæ—¨åœ¨å¸®åŠ©æ²¡æœ‰ç®—æ³•åŸºç¡€çš„å°ç™½é€šè¿‡ä¸€ä¸ªè¯¾ç¨‹å®Œæˆå¤§æ¨¡å‹å¼€å‘çš„åŸºç¡€å…¥é—¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåŠ å…¥ RAG å¼€å‘çš„è¿›é˜¶æŠ€å·§ä»¥åŠä¸€äº›æˆåŠŸçš„ LLM åº”ç”¨æ¡ˆä¾‹çš„è§£è¯»ï¼Œå¸®åŠ©å®Œæˆç¬¬ä¸€éƒ¨åˆ†å­¦ä¹ çš„è¯»è€…è¿›ä¸€æ­¥æŒæ¡æ›´é«˜é˜¶çš„ RAG å¼€å‘æŠ€å·§ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡å¯¹å·²æœ‰æˆåŠŸé¡¹ç›®çš„å€Ÿé‰´å¼€å‘è‡ªå·±çš„ã€å¥½ç©çš„åº”ç”¨ã€‚

***

## 4.1 LLMæ¥å…¥LangChain

LangChain ä¸ºåŸºäº LLM å¼€å‘è‡ªå®šä¹‰åº”ç”¨æä¾›äº†é«˜æ•ˆçš„å¼€å‘æ¡†æ¶ï¼Œä¾¿äºå¼€å‘è€…è¿…é€Ÿåœ°æ¿€å‘ LLM çš„å¼ºå¤§èƒ½åŠ›ï¼Œæ­å»º LLM åº”ç”¨ã€‚LangChain ä¹ŸåŒæ ·æ”¯æŒå¤šç§å¤§æ¨¡å‹ï¼Œå†…ç½®äº† OpenAIã€LLAMA ç­‰å¤§æ¨¡å‹çš„è°ƒç”¨æ¥å£ã€‚ä½†æ˜¯ï¼ŒLangChain å¹¶æ²¡æœ‰å†…ç½®æ‰€æœ‰å¤§æ¨¡å‹ï¼Œå®ƒé€šè¿‡å…è®¸ç”¨æˆ·è‡ªå®šä¹‰ LLM ç±»å‹ï¼Œæ¥æä¾›å¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚æ›´å¤šç»†èŠ‚å¯ä»¥æŸ¥çœ‹[Langchainå®˜æ–¹æ–‡æ¡£](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.chat_models)ã€‚

æˆ‘ä»¬åŒæ ·å¯ä»¥é€šè¿‡ LangChain æ¡†æ¶æ¥è°ƒç”¨æ™ºè°± AI å¤§æ¨¡å‹ï¼Œä»¥å°†å…¶æ¥å…¥åˆ°æˆ‘ä»¬çš„åº”ç”¨æ¡†æ¶ä¸­ã€‚ç”±äº langchain ä¸­æä¾›çš„[ChatGLM](https://python.langchain.com/docs/integrations/llms/chatglm)å·²ä¸å¯ç”¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ªLLMã€‚

æœ¬æ¬¡å­¦ä¹ æˆ‘ä½¿ç”¨çš„æ˜¯æ™ºè°± GLM APIï¼Œéœ€è¦å°†æˆ‘ä»¬å°è£…æ¥å£çš„ä»£ç è°ƒç”¨LangChainã€‚æ ¹æ®æ™ºè°±å®˜æ–¹å®£å¸ƒä»¥ä¸‹æ¨¡å‹å³å°†å¼ƒç”¨ï¼Œåœ¨è¿™äº›æ¨¡å‹å¼ƒç”¨åï¼Œä¼šå°†å®ƒä»¬è‡ªåŠ¨è·¯ç”±è‡³æ–°çš„æ¨¡å‹ã€‚è¯·ç”¨æˆ·æ³¨æ„åœ¨å¼ƒç”¨æ—¥æœŸä¹‹å‰ï¼Œå°†æ‚¨çš„æ¨¡å‹ç¼–ç æ›´æ–°ä¸ºæœ€æ–°ç‰ˆæœ¬ï¼Œä»¥ç¡®ä¿æœåŠ¡çš„é¡ºç•…è¿‡æ¸¡ï¼Œæ›´å¤šæ¨¡å‹ç›¸å…³ä¿¡æ¯è¯·è®¿é—®[model](https://open.bigmodel.cn/dev/howuse/model)

| æ¨¡å‹ç¼–ç  |å¼ƒç”¨æ—¥æœŸ|æŒ‡å‘æ¨¡å‹|
| ---- | ---- | ---- |
|chatglm_pro|2024 å¹´ 12 æœˆ 31 æ—¥|glm-4|
|chatglm_std|2024 å¹´ 12 æœˆ 31 æ—¥|glm-3-turbo|
|chatglm_lite|2024 å¹´ 12 æœˆ 31 æ—¥|glm-3-turbo|

## 4.2 å°è£…ä»£ç 

å°è£…ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from typing import Any, List, Mapping, Optional, Dict
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from zhipuai import ZhipuAI

# ç»§æ‰¿è‡ª langchain_core.language_models.llms.LLM
class ZhipuAILLM(LLM):
    # é»˜è®¤é€‰ç”¨ glm-4 æ¨¡å‹
    model: str = "glm-4"
    # æ¸©åº¦ç³»æ•°
    temperature: float = 0.1
    # API_Key
    api_key: str = None
    
    def _call(self, prompt : str, stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any):
        
        def gen_glm_params(prompt):
            '''
            æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages

            è¯·æ±‚å‚æ•°ï¼š
                prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯
            '''
            messages = [{"role": "user", "content": prompt}]
            return messages
        
        client = ZhipuAI(
            api_key=self.api_key
        )
     
        messages = gen_glm_params(prompt)
        response = client.chat.completions.create(
            model = self.model,
            messages = messages,
            temperature = self.temperature
        )

        if len(response.choices) > 0:
            return response.choices[0].message.content
        return "generate answer error"


    # é¦–å…ˆå®šä¹‰ä¸€ä¸ªè¿”å›é»˜è®¤å‚æ•°çš„æ–¹æ³•
    @property
    def _default_params(self) -> Dict[str, Any]:
        """è·å–è°ƒç”¨Ennie APIçš„é»˜è®¤å‚æ•°ã€‚"""
        normal_params = {
            "temperature": self.temperature,
            }
        # print(type(self.model_kwargs))
        return {**normal_params}

    @property
    def _llm_type(self) -> str:
        return "Zhipu"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {**{"model": self.model}, **self._default_params}
å…·ä½“ä½¿ç”¨ï¼š

```

```python
from zhipuai_llm import ZhipuAILLM

from dotenv import find_dotenv, load_dotenv
import os

_ = load_dotenv(find_dotenv())

api_key = os.environ["ZHIPUAI_API_KEY"] #å¡«å†™æ§åˆ¶å°ä¸­è·å–çš„ APIKey ä¿¡æ¯

zhipuai_model = ZhipuAILLM(model="chatglm_std", temperature=0, api_key=api_key)

zhipuai_model("ä½ å¥½ï¼Œè¯·ä½ è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹ï¼")

```

æˆåŠŸï¼

<img src="../picts/image-20240628175456469.png" style="zoom:100%;margin-left:0px;" />



## 4.3 æ„å»ºæ£€ç´¢é—®ç­”çŸ¥è¯†é“¾

è¿™ä¸€éƒ¨åˆ†éœ€è¦åŸºäº[ä»»åŠ¡ä¸‰]([Datawhale_LLM/notes/Third_Task.md at main Â· lyc686/Datawhale_LLM (github.com)](https://github.com/lyc686/Datawhale_LLM/blob/main/notes/Third_Task.md))ä¸­æ„å»ºå¥½çš„æœ¬åœ°çŸ¥è¯†çš„æ–‡æ¡£ï¼Œå»ºç«‹ä¸€ä¸ªå‘é‡çŸ¥è¯†åº“ï¼Œç„¶åå¯¹queryæŸ¥è¯¢é—®é¢˜è¿›è¡Œå¬å›ï¼Œå¹¶å°†å¬å›ç»“æœå’Œqueryç»“åˆæ„å»ºpromptï¼Œè¾“å…¥å¤§æ¨¡å‹å¾—åˆ°å›ç­”ã€‚

### 4.3.1 æ„å»ºå‘é‡æ•°æ®åº“

```python
import sys
# sys.path.append("../C3 æ­å»ºçŸ¥è¯†åº“") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­

# ä½¿ç”¨æ™ºè°± Embedding APIï¼Œæ³¨æ„ï¼Œéœ€è¦å°†ä¸Šä¸€ç« å®ç°çš„å°è£…ä»£ç ä¸‹è½½åˆ°æœ¬åœ°
from zhipuai_embedding import ZhipuAIEmbeddings

from langchain.vectorstores.chroma import Chroma

from dotenv import load_dotenv, find_dotenv
import os

_ = load_dotenv(find_dotenv())    # read local .env file
zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']

# å®šä¹‰ Embeddings
embedding = ZhipuAIEmbeddings()

# å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
persist_directory = '../../data_base/vector_db/chroma'

# åŠ è½½æ•°æ®åº“
vectordb = Chroma(
    persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
    embedding_function=embedding
)

print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}")
```

<img src="../picts/image-20240703102017512.png" style="zoom:90%;margin-left:0px;" />

è¾“å…¥ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦æŸ¥è¯¢çš„å†…å®¹è¿›è¡Œæµ‹è¯•ï¼š

```python
question = "defect detection"
docs = vectordb.similarity_search(question,k=3)
print(f"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(docs)}")

for i, doc in enumerate(docs):
    print(f"æ£€ç´¢åˆ°çš„ç¬¬{i}ä¸ªå†…å®¹: \n {doc.page_content}", end="\n-----------------------------------------------------\n")
```

<img src="../picts/image-20240703102220116.png" style="zoom:100%;margin-left:0px;" />

<img src="../picts/image-20240703102236260.png" style="zoom:100%;margin-left:0px;" />

### 4.3.2 åˆ›å»ºLLMæ„å»ºæ£€ç´¢é—®ç­”é“¾

åˆ›å»ºæ£€ç´¢ QA é“¾çš„æ–¹æ³• RetrievalQA.from_chain_type() æœ‰å¦‚ä¸‹å‚æ•°ï¼š
- llmï¼šæŒ‡å®šä½¿ç”¨çš„ LLM
- æŒ‡å®š chain type : RetrievalQA.from_chain_type(chain_type="map_reduce")ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨load_qa_chain()æ–¹æ³•æŒ‡å®šchain typeã€‚
- è‡ªå®šä¹‰ prompt ï¼šé€šè¿‡åœ¨RetrievalQA.from_chain_type()æ–¹æ³•ä¸­ï¼ŒæŒ‡å®šchain_type_kwargså‚æ•°ï¼Œè€Œè¯¥å‚æ•°ï¼šchain_type_kwargs = {"prompt": PROMPT}
- è¿”å›æºæ–‡æ¡£ï¼šé€šè¿‡RetrievalQA.from_chain_type()æ–¹æ³•ä¸­æŒ‡å®šï¼šreturn_source_documents=Trueå‚æ•°ï¼›ä¹Ÿå¯ä»¥ä½¿ç”¨RetrievalQAWithSourceChain()æ–¹æ³•ï¼Œè¿”å›æºæ–‡æ¡£çš„å¼•ç”¨ï¼ˆåæ ‡æˆ–è€…å«ä¸»é”®ã€ç´¢å¼•ï¼‰

```python
import os 
OPENAI_API_KEY = os.environ['ZHIPUAI_API_KEY']

# llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0)
llm  = ZhipuAILLM(model="chatglm_std", temperature=0, api_key=api_key)
llm.invoke("è¯·ä½ è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ï¼")
```

<img src="../picts/image-20240703102419407.png" style="zoom:100%;margin-left:0px;" />

```python
from langchain.prompts import PromptTemplate

template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
{context}
é—®é¢˜: {question}
"""

QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                                 template=template)

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
```

åŸºäºæˆ‘ä»¬çš„æœ¬åœ°çŸ¥è¯†åº“çš„å†…å®¹æ„å»ºä¸¤ä¸ªé—®é¢˜:

```python
question_1 = "ä»€ä¹ˆæ˜¯ghostå·ç§¯ï¼Ÿ"

result = qa_chain({"query": question_1})
print("å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_1 çš„ç»“æœï¼š")
print(result["result"])
```

<img src="../picts/image-20240703102625131.png" style="zoom:100%;margin-left:0px;" />

```python
question_2 = "å…‰ä¼ç”µæ± çš„ç¼ºé™·æ£€æµ‹æ–¹æ³•å¯ä»¥åˆ†ä¸ºå“ªå‡ ç§ï¼Ÿ"

result = qa_chain({"query": question_2})
print("å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_2 çš„ç»“æœï¼š")
print(result["result"])
```

<img src="../picts/image-20240703102635322.png" style="zoom:100%;margin-left:0px;" />

åŒæ—¶æˆ‘ä»¬ä¹Ÿå¯ä»¥å•çº¯çš„åªåŸºäºå¤§æ¨¡å‹æ¥è¿›è¡Œå›ç­”ï¼š

```python
prompt_template = """è¯·å›ç­”ä¸‹åˆ—é—®é¢˜:
                            {}""".format(question_1)

### åŸºäºå¤§æ¨¡å‹çš„é—®ç­”
llm.predict(prompt_template)
```

<img src="../picts/image-20240703102742385.png" style="zoom:100%;margin-left:0px;" />

```python
prompt_template = """è¯·å›ç­”ä¸‹åˆ—é—®é¢˜:
                            {}""".format(question_2)

### åŸºäºå¤§æ¨¡å‹çš„é—®ç­”
llm.predict(prompt_template)
```

<img src="../picts/image-20240703102757340.png" style="zoom:100%;margin-left:0px;" />

è§‚å¯Ÿå‘ç°æ·»åŠ äº†æœ¬åœ°çš„çŸ¥è¯†åº“ä¹‹åï¼Œæˆ‘ä»¬çš„å›ç­”ä¼šæ›´åŠ è´´è¿‘æˆ‘ä»¬æœŸæœ›çš„ç»“æœã€‚

> â­ é€šè¿‡ä»¥ä¸Šä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å‘ç° LLM å¯¹äºä¸€äº›è¿‘å‡ å¹´çš„çŸ¥è¯†ä»¥åŠéå¸¸è¯†æ€§çš„ä¸“ä¸šé—®é¢˜ï¼Œå›ç­”çš„å¹¶ä¸æ˜¯å¾ˆå¥½ã€‚è€ŒåŠ ä¸Šæˆ‘ä»¬çš„æœ¬åœ°çŸ¥è¯†ï¼Œå°±å¯ä»¥å¸®åŠ© LLM åšå‡ºæ›´å¥½çš„å›ç­”ã€‚å¦å¤–ï¼Œä¹Ÿæœ‰åŠ©äºç¼“è§£å¤§æ¨¡å‹çš„â€œå¹»è§‰â€é—®é¢˜ã€‚

### 4.3.3 æ·»åŠ å†å²å¯¹è¯çš„è®°å¿†åŠŸèƒ½

ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†é€šè¿‡ä¸Šä¼ æœ¬åœ°çŸ¥è¯†æ–‡æ¡£ï¼Œç„¶åå°†ä»–ä»¬ä¿å­˜åˆ°å‘é‡çŸ¥è¯†åº“ï¼Œé€šè¿‡å°†æŸ¥è¯¢é—®é¢˜ä¸å‘é‡çŸ¥è¯†åº“çš„å¬å›ç»“æœè¿›è¡Œç»“åˆè¾“å…¥åˆ° LLM ä¸­ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªç›¸æ¯”äºç›´æ¥è®© LLM å›ç­”è¦å¥½å¾—å¤šçš„ç»“æœã€‚åœ¨ä¸è¯­è¨€æ¨¡å‹äº¤äº’æ—¶ï¼Œä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ä¸€ä¸ªå…³é”®é—®é¢˜ - **å®ƒä»¬å¹¶ä¸è®°å¾—ä½ ä¹‹å‰çš„äº¤æµå†…å®¹**ã€‚è¿™åœ¨æˆ‘ä»¬æ„å»ºä¸€äº›åº”ç”¨ç¨‹åºï¼ˆå¦‚èŠå¤©æœºå™¨äººï¼‰çš„æ—¶å€™ï¼Œå¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œä½¿å¾—å¯¹è¯ä¼¼ä¹ç¼ºä¹çœŸæ­£çš„è¿ç»­æ€§ã€‚

#### ä¸€ã€åµŒå…¥è®°å¿†

åœ¨æœ¬èŠ‚ä¸­æˆ‘ä»¬å°†ä»‹ç» LangChain ä¸­çš„å‚¨å­˜æ¨¡å—ï¼Œå³å¦‚ä½•å°†å…ˆå‰çš„å¯¹è¯åµŒå…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­çš„ï¼Œä½¿å…¶å…·æœ‰è¿ç»­å¯¹è¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `ConversationBufferMemory` ï¼Œå®ƒä¿å­˜èŠå¤©æ¶ˆæ¯å†å²è®°å½•çš„åˆ—è¡¨ï¼Œè¿™äº›å†å²è®°å½•å°†åœ¨å›ç­”é—®é¢˜æ—¶ä¸é—®é¢˜ä¸€èµ·ä¼ é€’ç»™èŠå¤©æœºå™¨äººï¼Œä»è€Œå°†å®ƒä»¬æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ã€‚

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚
    return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
)
```

å…³äºæ›´å¤šçš„ Memory çš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬ä¿ç•™æŒ‡å®šå¯¹è¯è½®æ•°ã€ä¿å­˜æŒ‡å®š token æ•°é‡ã€ä¿å­˜å†å²å¯¹è¯çš„æ€»ç»“æ‘˜è¦ç­‰å†…å®¹ï¼Œè¯·å‚è€ƒ langchain çš„ Memory éƒ¨åˆ†çš„ç›¸å…³æ–‡æ¡£ã€‚

#### äºŒã€æ„å»ºå¯¹è¯æ£€ç´¢é“¾

å¯¹è¯æ£€ç´¢é“¾ï¼ˆConversationalRetrievalChainï¼‰åœ¨æ£€ç´¢ QA é“¾çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†å¤„ç†å¯¹è¯å†å²çš„èƒ½åŠ›ã€‚

å®ƒçš„å·¥ä½œæµç¨‹æ˜¯:
1. å°†ä¹‹å‰çš„å¯¹è¯ä¸æ–°é—®é¢˜åˆå¹¶ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æŸ¥è¯¢è¯­å¥ã€‚
2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢è¯¥æŸ¥è¯¢çš„ç›¸å…³æ–‡æ¡£ã€‚
3. è·å–ç»“æœå,å­˜å‚¨æ‰€æœ‰ç­”æ¡ˆåˆ°å¯¹è¯è®°å¿†åŒºã€‚
4. ç”¨æˆ·å¯åœ¨ UI ä¸­æŸ¥çœ‹å®Œæ•´çš„å¯¹è¯æµç¨‹ã€‚

è¿™ç§é“¾å¼æ–¹å¼å°†æ–°é—®é¢˜æ”¾åœ¨ä¹‹å‰å¯¹è¯çš„è¯­å¢ƒä¸­è¿›è¡Œæ£€ç´¢ï¼Œå¯ä»¥å¤„ç†ä¾èµ–å†å²ä¿¡æ¯çš„æŸ¥è¯¢ã€‚å¹¶ä¿ç•™æ‰€æœ‰ä¿¡

æ¯åœ¨å¯¹è¯è®°å¿†ä¸­ï¼Œæ–¹ä¾¿è¿½è¸ªã€‚

ä½¿ç”¨ä¸Šä¸€éƒ¨åˆ†ä¸­çš„å‘é‡æ•°æ®åº“å’Œ LLM ï¼é¦–å…ˆæå‡ºä¸€ä¸ªæ— å†å²å¯¹è¯çš„é—®é¢˜â€œè¿™é—¨è¯¾ä¼šå­¦ä¹ åˆ°å…³äºæç¤ºå·¥ç¨‹çš„çŸ¥è¯†å—ï¼Ÿâ€ï¼Œå¹¶æŸ¥çœ‹å›ç­”ã€‚

```python
from langchain.chains import ConversationalRetrievalChain

retriever=vectordb.as_retriever()

qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)
question = "æˆ‘å¯ä»¥å­¦ä¹ åˆ°å…³äºæç¤ºå·¥ç¨‹çš„çŸ¥è¯†å—ï¼Ÿ"
result = qa({"question": question})
print(result['answer'])
```

<img src="../picts/image-20240703103248250.png" style="zoom:100%;margin-left:0px;" />

ç„¶ååŸºäºç­”æ¡ˆå¯¹ä¸‹ä¸€ä¸ªé—®é¢˜è¿›è¡Œç»§ç»­è®¨è®ºï¼š

```python
question = "ä¸ºä»€ä¹ˆè¿™é—¨è¯¾éœ€è¦æ•™è¿™æ–¹é¢çš„çŸ¥è¯†ï¼Ÿ"
result = qa({"question": question})
print(result['answer'])
```

<img src="../picts/image-20240703103329433.png" style="zoom:100%;margin-left:0px;" />

å¯ä»¥çœ‹åˆ°ï¼ŒLLM å®ƒå‡†ç¡®åœ°åˆ¤æ–­äº†è¿™æ–¹é¢çš„çŸ¥è¯†ï¼ŒæŒ‡ä»£å†…å®¹æ˜¯å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†ï¼Œä¹Ÿå°±

æ˜¯æˆ‘ä»¬æˆåŠŸåœ°ä¼ é€’ç»™äº†å®ƒå†å²ä¿¡æ¯ã€‚è¿™ç§æŒç»­å­¦ä¹ å’Œå…³è”å‰åé—®é¢˜çš„èƒ½åŠ›ï¼Œå¯å¤§å¤§å¢å¼ºé—®ç­”ç³»ç»Ÿçš„è¿ç»­

æ€§å’Œæ™ºèƒ½æ°´å¹³ã€‚

## 4.4 éƒ¨ç½²çŸ¥è¯†é—®ç­”å°åŠ©æ‰‹

### 4.4.1 streamlitå·¥å…·åŒ…

Streamlit æ˜¯ä¸€ç§å¿«é€Ÿä¾¿æ·çš„æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥åœ¨ **Python ä¸­é€šè¿‡å‹å¥½çš„ Web ç•Œé¢æ¼”ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹**ã€‚åœ¨æœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ **å¦‚ä½•ä½¿ç”¨å®ƒä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºæ„å»ºç”¨æˆ·ç•Œé¢**ã€‚åœ¨æ„å»ºäº†æœºå™¨å­¦ä¹ æ¨¡å‹åï¼Œå¦‚æœä½ æƒ³æ„å»ºä¸€ä¸ª demo ç»™å…¶ä»–äººçœ‹ï¼Œä¹Ÿè®¸æ˜¯ä¸ºäº†è·å¾—åé¦ˆå¹¶æ¨åŠ¨ç³»ç»Ÿçš„æ”¹è¿›ï¼Œæˆ–è€…åªæ˜¯å› ä¸ºä½ è§‰å¾—è¿™ä¸ªç³»ç»Ÿå¾ˆé…·ï¼Œæ‰€ä»¥æƒ³æ¼”ç¤ºä¸€ä¸‹ï¼šStreamlit å¯ä»¥è®©æ‚¨é€šè¿‡ Python æ¥å£ç¨‹åºå¿«é€Ÿå®ç°è¿™ä¸€ç›®æ ‡ï¼Œè€Œæ— éœ€ç¼–å†™ä»»ä½•å‰ç«¯ã€ç½‘é¡µæˆ– JavaScript ä»£ç ã€‚




`Streamlit` æ˜¯ä¸€ä¸ªç”¨äºå¿«é€Ÿåˆ›å»ºæ•°æ®åº”ç”¨ç¨‹åºçš„å¼€æº Python åº“ã€‚å®ƒçš„è®¾è®¡ç›®æ ‡æ˜¯è®©æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿè½»æ¾åœ°å°†æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹è½¬åŒ–ä¸ºå…·æœ‰äº¤äº’æ€§çš„ Web åº”ç”¨ç¨‹åºï¼Œè€Œæ— éœ€æ·±å…¥äº†è§£ Web å¼€å‘ã€‚å’Œå¸¸è§„ Web æ¡†æ¶ï¼Œå¦‚ Flask/Django çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä¸éœ€è¦ä½ å»ç¼–å†™ä»»ä½•å®¢æˆ·ç«¯ä»£ç ï¼ˆHTML/CSS/JSï¼‰ï¼Œåªéœ€è¦ç¼–å†™æ™®é€šçš„ Python æ¨¡å—ï¼Œå°±å¯ä»¥åœ¨å¾ˆçŸ­çš„æ—¶é—´å†…åˆ›å»ºç¾è§‚å¹¶å…·å¤‡é«˜åº¦äº¤äº’æ€§çš„ç•Œé¢ï¼Œä»è€Œå¿«é€Ÿç”Ÿæˆæ•°æ®åˆ†ææˆ–è€…æœºå™¨å­¦ä¹ çš„ç»“æœï¼›å¦ä¸€æ–¹é¢ï¼Œå’Œé‚£äº›åªèƒ½é€šè¿‡æ‹–æ‹½ç”Ÿæˆçš„å·¥å…·ä¹Ÿä¸åŒçš„æ˜¯ï¼Œä½ ä»ç„¶å…·æœ‰å¯¹ä»£ç çš„å®Œæ•´æ§åˆ¶æƒã€‚

Streamlit æä¾›äº†ä¸€ç»„ç®€å•è€Œå¼ºå¤§çš„åŸºç¡€æ¨¡å—ï¼Œç”¨äºæ„å»ºæ•°æ®åº”ç”¨ç¨‹åºï¼š

- st.write()ï¼šè¿™æ˜¯æœ€åŸºæœ¬çš„æ¨¡å—ä¹‹ä¸€ï¼Œç”¨äºåœ¨åº”ç”¨ç¨‹åºä¸­å‘ˆç°æ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼ç­‰å†…å®¹ã€‚

- st.title()ã€st.header()ã€st.subheader()ï¼šè¿™äº›æ¨¡å—ç”¨äºæ·»åŠ æ ‡é¢˜ã€å­æ ‡é¢˜å’Œåˆ†ç»„æ ‡é¢˜ï¼Œä»¥ç»„ç»‡åº”ç”¨ç¨‹åºçš„å¸ƒå±€ã€‚

- st.text()ã€st.markdown()ï¼šç”¨äºæ·»åŠ æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒ Markdown è¯­æ³•ã€‚

- st.image()ï¼šç”¨äºæ·»åŠ å›¾åƒåˆ°åº”ç”¨ç¨‹åºä¸­ã€‚

- st.dataframe()ï¼šç”¨äºå‘ˆç° Pandas æ•°æ®æ¡†ã€‚

- st.table()ï¼šç”¨äºå‘ˆç°ç®€å•çš„æ•°æ®è¡¨æ ¼ã€‚

- st.pyplot()ã€st.altair_chart()ã€st.plotly_chart()ï¼šç”¨äºå‘ˆç° Matplotlibã€Altair æˆ– Plotly ç»˜åˆ¶çš„å›¾è¡¨ã€‚

- st.selectbox()ã€st.multiselect()ã€st.slider()ã€st.text_input()ï¼šç”¨äºæ·»åŠ äº¤äº’å¼å°éƒ¨ä»¶ï¼Œå…è®¸ç”¨æˆ·åœ¨åº”ç”¨ç¨‹åºä¸­è¿›è¡Œé€‰æ‹©ã€è¾“å…¥æˆ–æ»‘åŠ¨æ“ä½œã€‚

- st.button()ã€st.checkbox()ã€st.radio()ï¼šç”¨äºæ·»åŠ æŒ‰é’®ã€å¤é€‰æ¡†å’Œå•é€‰æŒ‰é’®ï¼Œä»¥è§¦å‘ç‰¹å®šçš„æ“ä½œã€‚

è¿™äº›åŸºç¡€æ¨¡å—ä½¿å¾—é€šè¿‡ Streamlit èƒ½å¤Ÿè½»æ¾åœ°æ„å»ºäº¤äº’å¼æ•°æ®åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨æ—¶å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œç»„åˆå’Œå®šåˆ¶ï¼Œæ›´å¤šå†…å®¹è¯·æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](https://docs.streamlit.io/get-started)



```python
import streamlit as st
from langchain_openai import ChatOpenAI
import os
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
# import sys
# sys.path.append("../C3 æ­å»ºçŸ¥è¯†åº“") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())    # read local .env file


from typing import Any, List, Mapping, Optional, Dict
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from zhipuai import ZhipuAI

# ç»§æ‰¿è‡ª langchain_core.language_models.llms.LLM
class ZhipuAILLM(LLM):
    # é»˜è®¤é€‰ç”¨ glm-4 æ¨¡å‹
    model: str = "glm-4"
    # æ¸©åº¦ç³»æ•°
    temperature: float = 0.1
    # API_Key
    api_key: str = None
    
    def _call(self, prompt : str, stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any):
        
        def gen_glm_params(prompt):
            '''
            æ„é€  GLM æ¨¡å‹è¯·æ±‚å‚æ•° messages

            è¯·æ±‚å‚æ•°ï¼š
                prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯
            '''
            messages = [{"role": "user", "content": prompt}]
            return messages
        
        client = ZhipuAI(
            api_key=self.api_key
        )
     
        messages = gen_glm_params(prompt)
        response = client.chat.completions.create(
            model = self.model,
            messages = messages,
            temperature = self.temperature
        )

        if len(response.choices) > 0:
            return response.choices[0].message.content
        return "generate answer error"


    # é¦–å…ˆå®šä¹‰ä¸€ä¸ªè¿”å›é»˜è®¤å‚æ•°çš„æ–¹æ³•
    @property
    def _default_params(self) -> Dict[str, Any]:
        """è·å–è°ƒç”¨Ennie APIçš„é»˜è®¤å‚æ•°ã€‚"""
        normal_params = {
            "temperature": self.temperature,
            }
        # print(type(self.model_kwargs))
        return {**normal_params}

    @property
    def _llm_type(self) -> str:
        return "Zhipu"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {**{"model": self.model}, **self._default_params}


#export OPENAI_API_KEY=
#os.environ["OPENAI_API_BASE"] = 'https://api.chatgptid.net/v1'
zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']


def generate_response(input_text, zhipuai_api_key):
    llm  = ZhipuAILLM(model="chatglm_std", temperature=0, api_key=zhipuai_api_key)
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    #st.info(output)
    return output

def get_vectordb():
    # å®šä¹‰ Embeddings
    embedding = ZhipuAIEmbeddings()
    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    persist_directory = '../../data_base/vector_db/chroma'
    # åŠ è½½æ•°æ®åº“
    vectordb = Chroma(
        persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
        embedding_function=embedding
    )
    return vectordb

#å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
def get_chat_qa_chain(question:str,zhipuai_api_key:str):
    vectordb = get_vectordb()
    llm  = ZhipuAILLM(model="chatglm_std", temperature=0, api_key=zhipuai_api_key)
    memory = ConversationBufferMemory(
        memory_key="chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚
        return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
    )
    retriever=vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory
    )
    result = qa({"question": question})
    return result['answer']

#ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
def get_qa_chain(question:str,zhipuai_api_key:str):
    vectordb = get_vectordb()
    llm  = ZhipuAILLM(model="chatglm_std", temperature=0, api_key=zhipuai_api_key)
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                                 template=template)
    qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
    result = qa_chain({"query": question})
    return result["result"]


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    st.title('ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')
    api_key = st.sidebar.text_input('ZHIPUAI API Key', type='password')

    # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹
    #selected_method = st.sidebar.selectbox("é€‰æ‹©æ¨¡å¼", ["qa_chain", "chat_qa_chain", "None"])
    selected_method = st.radio(
        "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
        ["None", "qa_chain", "chat_qa_chain"],
        captions = ["ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼", "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"])

    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    messages = st.container(height=300)
    if prompt := st.chat_input("Say something"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append({"role": "user", "text": prompt})

        if selected_method == "None":
            # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
            answer = generate_response(prompt, api_key)
        elif selected_method == "qa_chain":
            answer = get_qa_chain(prompt,api_key)
        elif selected_method == "chat_qa_chain":
            answer = get_chat_qa_chain(prompt,api_key)

        # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
        if answer is not None:
            # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            st.session_state.messages.append({"role": "assistant", "text": answer})

        # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
        for message in st.session_state.messages:
            if message["role"] == "user":
                messages.chat_message("user").write(message["text"])
            elif message["role"] == "assistant":
                messages.chat_message("assistant").write(message["text"])   


if __name__ == "__main__":
    main()
```

æœ€ç»ˆé€šè¿‡æŒ‡ä»¤`streamlit run XXX.py`å³å¯è¿è¡Œã€‚æ•ˆæœå¦‚ä¸‹ï¼š

<img src="../picts/image-20240703112025285.png" style="zoom:100%;" />

### 4.4.2 éƒ¨ç½²åº”ç”¨ç¨‹åº

è¦å°†åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Streamlit Cloudï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š  

 

1. ä¸ºåº”ç”¨ç¨‹åºåˆ›å»º GitHub å­˜å‚¨åº“ã€‚æ‚¨çš„å­˜å‚¨åº“åº”åŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼š  

 

  your-repository/  

  â”œâ”€â”€ streamlit_app.py  

  â””â”€â”€ requirements.txt  

 

2. è½¬åˆ° [Streamlit Community Cloud](http://share.streamlit.io/)ï¼Œå•å‡»å·¥ä½œåŒºä¸­çš„`New app`æŒ‰é’®ï¼Œç„¶åæŒ‡å®šå­˜å‚¨åº“ã€åˆ†æ”¯å’Œä¸»æ–‡ä»¶è·¯å¾„ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡é€‰æ‹©è‡ªå®šä¹‰å­åŸŸæ¥è‡ªå®šä¹‰åº”ç”¨ç¨‹åºçš„ URL

 

3. ç‚¹å‡»`Deploy!`æŒ‰é’®  

 

æ‚¨çš„åº”ç”¨ç¨‹åºç°åœ¨å°†éƒ¨ç½²åˆ° Streamlit Community Cloudï¼Œå¹¶ä¸”å¯ä»¥ä»ä¸–ç•Œå„åœ°è®¿é—®ï¼ ğŸŒ  



é¡¹ç›®éƒ¨ç½²åˆ°è¿™åŸºæœ¬å®Œæˆï¼Œä¸ºäº†æ–¹ä¾¿è¿›è¡Œæ¼”ç¤ºè¿›è¡Œäº†ç®€åŒ–ï¼Œè¿˜æœ‰å¾ˆå¤šå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–çš„åœ°æ–¹ï¼ŒæœŸå¾…å­¦ä¹ è€…ä»¬è¿›è¡Œå„ç§é­”æ”¹ï¼

ä¼˜åŒ–æ–¹å‘ï¼š
- ç•Œé¢ä¸­æ·»åŠ ä¸Šä¼ æœ¬åœ°æ–‡æ¡£ï¼Œå»ºç«‹å‘é‡æ•°æ®åº“çš„åŠŸèƒ½
- æ·»åŠ å¤šç§LLM ä¸ embeddingæ–¹æ³•é€‰æ‹©çš„æŒ‰é’®
- æ·»åŠ ä¿®æ”¹å‚æ•°çš„æŒ‰é’®
- æ›´å¤š......
